{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLAgent: Automating tasks on Neuroglancer\n",
    "\n",
    "This code provides the infrastructure to use Neuroglancer as a fully automated agent. This heavily relies on the Selenium library and JavaScript to control Neuroglancer's state. This provides a flexible and fast way to interact with Neuroglancer, with possibilities to do headless, local or remote sessions. More importantly, it allows to simulate a human's interaction and have a real environment for Reinforcement Learning training.\n",
    "\n",
    "## The main kind of interaction is to:\n",
    "- **click** (left, right, middle, double) at a specific position\n",
    "- change the **JSON state** programmatically to navigate on continous dimensions (zoom, rotation, translation), add new layers, etc.\n",
    "\n",
    "## It can:\n",
    "- follow a recorded of sequence of clicks and JSON state changes done by a human\n",
    "- acquire screenshots in less than 5 ms\n",
    "- prepares the infrastructure to use Reinforcement Learning to navigate the Neuroglancer UI. However, this task is technically challenging. And has been decomposed into smaller tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have the following dependencies installed:\n",
    "\n",
    "- **Python**:(Version used: **3.10.16**)\n",
    "- **Selenium**: (Version used: **4.29.0**)\n",
    "- **Pillow**: (Version used: **11.1.0**)\n",
    "- **numpy**: (Version used: **2.2.4**)\n",
    "- **scipy**: (Version used: **1.12.1**)\n",
    "- **torch**\n",
    "\n",
    "We have provided already two chrome drivers for MacOS and Linux. It is very probably that you will need to install the correct version for your system. For this, you need to access Chrome and find your current version and then download the correct driver [here](https://googlechromelabs.github.io/chrome-for-testing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroglancer Automation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main class is environment.py which includes all necessary libraries to interact with ChromeNGL and Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Environment\n",
      "File \u001b[0;32m~/Documents/neuroglancer-benchmark/LLM_Proofreading_Agent/environment.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mStandalone full Neuroglancer environment. Handles sending actions, getting state, changing JSON state, parsing data, screenshotting, etc.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"------------Imports-------------\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_action\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mcopy\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01margparse\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mplatform\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mbase64\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01murllib\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting a new Session\n",
    "print(\"Starting session...\")\n",
    "env = Environment(headless=False)\n",
    "env.start_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current state\n",
    "(pos_state, curr_image), json_state = env.prepare_state()\n",
    "print(f\"Position state: {pos_state}\")\n",
    "print(f\"Image size: {curr_image.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting Programmatically with `Agent.apply_actions()`\n",
    "\n",
    "The `Agent.apply_actions()` function is designed to handle interactions by applying incremental changes to a JSON structure and specifying raw positions for mouse clicks.\n",
    "\n",
    "### Action Vector Structure\n",
    "\n",
    "The function takes a vector with the following components:\n",
    "\n",
    "```plaintext\n",
    "(\n",
    "    left_click, right_click, double_click,  # 3 booleans for mouse clicks\n",
    "    x, y,                                  # 2 floats for absolute mouse position\n",
    "    key_Shift, key_Ctrl, key_Alt,          # 3 booleans for modifier keys\n",
    "    json_change,                           # 1 boolean indicating a JSON change\n",
    "    delta_position_x, delta_position_y, delta_position_z,  # 3 floats for position deltas\n",
    "    delta_crossSectionScale,               # 1 float for cross-section scaling\n",
    "    delta_projectionOrientation_q1, delta_projectionOrientation_q2,\n",
    "    delta_projectionOrientation_q3, delta_projectionOrientation_q4,  # 4 floats for orientation quaternion\n",
    "    delta_projectionScale                  # 1 float for projection scaling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample action vector\n",
    "action_vector = [\n",
    "    0, 0, 0,  # left, right, double click (all False)\n",
    "    100, 100,  # x, y mouse position\n",
    "    0, 0, 0,  # key_Shift, key_Ctrl, key_Alt (all False)\n",
    "    1,  # json_change = True, meaning we want to modify the JSON state\n",
    "    10, 0, 0,  # delta_position_x, delta_position_y, delta_position_z\n",
    "    0,  # delta_crossSectionScale\n",
    "    0.1, 0, 0, 0,  # delta_projectionOrientation_q1, q2, q3, q4\n",
    "    1000  # delta_projectionScale\n",
    "]\n",
    "#Applying the action\n",
    "print(\"Applying action...\")\n",
    "env.apply_actions(action_vector)\n",
    "\n",
    "# Wait a moment to see the effect\n",
    "time.sleep(1)\n",
    "\n",
    "# Clean up\n",
    "env.end_session()\n",
    "print(\"Environment session ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an environment test, we can define a loop. And use a model to take the action decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example loop\n",
    "for i in range(100):\n",
    "    # Get the current state\n",
    "    (pos_state, curr_image), json_state = env.prepare_state(\n",
    "        image_path=None, \n",
    "        euler_angles=True, \n",
    "        resize=False, \n",
    "        add_mouse=False, \n",
    "        fast=True\n",
    "    )\n",
    "    \n",
    "    # For example purposes, using a predefined action vector\n",
    "    action_vector = [\n",
    "        0, 0, 0,  # left, right, double click booleans\n",
    "        100, 100,  # x, y mouse position\n",
    "        0, 0, 0,  # no modifier keys\n",
    "        1,  # json_change = True\n",
    "        10, 0, 0,  # position change\n",
    "        0,  # cross-section scaling\n",
    "        0.2, 0, 0,  # orientation change in Euler angles\n",
    "        2000  # projection scaling (log-scale in neuroglancer)\n",
    "    ]\n",
    "    \n",
    "    # Apply the action\n",
    "    env.apply_actions(action_vector, json_state=json_state, euler_angles=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also follow an episode that has been recorded by a human\n",
    "episode_path = \"./episodes/sample_episode.json\"\n",
    "env.follow_episode(episode_path, sleep_time=0.1)\n",
    "\n",
    "#If we need to parse an episode from its recorded sequence of JSON states and actions, we can use the parse_episode function.\n",
    "episode_path = \"./episodes/raw/1800x900/episode_5.json\"\n",
    "save_path = \"./reparsed_episodes/test/\"\n",
    "parsed_data = env.parse_episode(episode_path, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Framework: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = Environment(headless=False, verbose=True)\n",
    "env.start_session()\n",
    "\n",
    "# Define action vector\n",
    "action_vector = [\n",
    "    0, 0, 0,  # left, right, double click\n",
    "    100, 100,  # x, y\n",
    "    0, 0, 0,  # no modifier keys\n",
    "    1,  # json_change = True\n",
    "    10, 0, 0,  # position change\n",
    "    0,  # cross-section scaling\n",
    "    0.1, 0, 0, 0,  # orientation change\n",
    "    1000  # projection scaling\n",
    "]\n",
    "\n",
    "# Take a step\n",
    "state, reward, done, json_state = env.step(action_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
