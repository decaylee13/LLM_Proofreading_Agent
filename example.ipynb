{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLAgent: Automating tasks on Neuroglancer\n",
    "\n",
    "This code provides the infrastructure to use Neuroglancer as a fully automated agent. This heavily relies on the Selenium library and JavaScript to control Neuroglancer's state. This provides a flexible and fast way to interact with Neuroglancer, with possibilities to do headless, local or remote sessions. More importantly, it allows to simulate a human's interaction and have a real environment for Reinforcement Learning training.\n",
    "\n",
    "## The main kind of interaction is to:\n",
    "- **click** (left, right, middle, double) at a specific position\n",
    "- change the **JSON state** programmatically to navigate on continous dimensions (zoom, rotation, translation), add new layers, etc.\n",
    "\n",
    "## It can:\n",
    "- follow a recorded of sequence of clicks and JSON state changes done by a human\n",
    "- acquire screenshots in less than 5 ms\n",
    "- prepares the infrastructure to use Reinforcement Learning to navigate the Neuroglancer UI. However, this task is technically challenging. And has been decomposed into smaller tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of the agent following a human's interaction by reiterating the actions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video width=\"900\" height=\"600\" controls>\n",
    "  <source src=\"tutorial_images/agent_test.mov\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have the following dependencies installed:\n",
    "\n",
    "- **Python**:(Version used: **3.10.16**)\n",
    "- **Selenium**: (Version used: **4.29.0**)\n",
    "- **Pillow**: (Version used: **11.1.0**)\n",
    "- **numpy**: (Version used: **2.2.4**)\n",
    "- **scipy**: (Version used: **1.12.1**)\n",
    "- **torch**\n",
    "\n",
    "We have provided already two chrome drivers for MacOS and Linux. It is very probably that you will need to install the correct version for your system. For this, you need to access Chrome and find your current version and then download the correct driver [here](https://googlechromelabs.github.io/chrome-for-testing/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroglancer Automation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main classes are <span style=\"color:blue; font-weight:bold;\">Agent</span> and <span style=\"color:green; font-weight:bold;\">ChromeNGL</span>.\n",
    "\n",
    "The <span style=\"color:blue; font-weight:bold;\">Agent</span> class is the main class that is used to interact with the Neuroglancer UI.\n",
    "\n",
    "The <span style=\"color:green; font-weight:bold;\">ChromeNGL</span> class is used to control the Chrome browser and the Neuroglancer UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agent import Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will open a Chrome browser. With headless=True, it will not be visible but all functions will still be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login attempted. Waiting for confirmation...\n",
      "Login successful!\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(start_session=True, headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.start_neuroglancer_session() #defaults to neuroglancer-demo.appspot.com and a default JSON state\n",
    "# For graphene or other middle-auth+ sessions, we need to start a local host session with the URL we want to start from. This function does the extra google auth login.\n",
    "#agent.start_graphene_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a Neuroglancer session we can interact with it.\n",
    "\n",
    "For convenience, we define a pos_state variable that contains the position, crossSectionScale, projectionOrientation, projectionScale.\n",
    "\n",
    "The observed state == state here and is: 'state = (pos_state, curr_image).'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[143944.703125, 61076.59375, 192.5807647705078], 2.0339912586467497, [-0.4705163836479187, 0.8044001460075378, -0.30343097448349, 0.1987067461013794], 13976.00585680798]\n",
      "Size of current image:  (1046, 992)\n"
     ]
    }
   ],
   "source": [
    "pos_state, curr_image, json_state = agent.prepare_state(image_path=None, euler_angles=False, resize=False, add_mouse=False, fast=True)\n",
    "# if only the state is needed, we can use the get_state function\n",
    "#pos_state = agent.get_state(euler_angles=False)\n",
    "\n",
    "print(pos_state) # list of floats and ints\n",
    "print(\"Size of current image: \", curr_image.size)\n",
    "#curr_image.show() # PIL image of the current state\n",
    "#print(json_state) # JSON state of the current state as a dictionary for easy access to this state in the future if needed\n",
    "# change url with agent.chrome_ngl.change_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting Programmatically with `Agent.apply_actions()`\n",
    "\n",
    "The `Agent.apply_actions()` function is designed to handle interactions by applying incremental changes to a JSON structure and specifying raw positions for mouse clicks.\n",
    "\n",
    "### Action Vector Structure\n",
    "\n",
    "The function takes a vector with the following components:\n",
    "\n",
    "```plaintext\n",
    "(\n",
    "    left_click, right_click, double_click,  # 3 booleans for mouse clicks\n",
    "    x, y,                                  # 2 floats for absolute mouse position\n",
    "    key_Shift, key_Ctrl, key_Alt,          # 3 booleans for modifier keys\n",
    "    json_change,                           # 1 boolean indicating a JSON change\n",
    "    delta_position_x, delta_position_y, delta_position_z,  # 3 floats for position deltas\n",
    "    delta_crossSectionScale,               # 1 float for cross-section scaling\n",
    "    delta_projectionOrientation_q1, delta_projectionOrientation_q2,\n",
    "    delta_projectionOrientation_q3, delta_projectionOrientation_q4,  # 4 floats for orientation quaternion\n",
    "    delta_projectionScale                  # 1 float for projection scaling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a click action takes priority over any JSON change. JSON change will be ignored if a click action is present. Very easy to modify, this was done to have easier training.\n",
    "action_vector = [\n",
    "    0, 0, 0,  # left, right, double click\n",
    "    100, 100,  # x, y\n",
    "    0, 0, 0,  # no modifier keys\n",
    "    1,  # no JSON change\n",
    "    10, 0, 0,  # position change\n",
    "    0,  # cross-section scaling\n",
    "    0.1, 0, 0, 0,  # orientation change default is quaternion\n",
    "    1000  # projection scaling (log-scale in neuroglancer)\n",
    "] # \n",
    "#print(action_vector)\n",
    "agent.apply_actions(action_vector, euler_angles=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an environment test, we can define a loop. And use a model to take the action decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExampleLoop\n",
    "\n",
    "for i in range(100):\n",
    "    pos_state, curr_image, json_state = agent.prepare_state(image_path=None, euler_angles=True, resize=False, add_mouse=False, fast=True) # time is about 0.05 seconds\n",
    "    # --> action_vector = model.predict(pos_state, curr_image)\n",
    "    # For example purpose, lets use a random action\n",
    "    action_vector = [\n",
    "        0, 0, 0,  # left, right, double click booleans\n",
    "        100, 100,  # x, y\n",
    "        0, 0, 0,  # no modifier keys\n",
    "        1,  # no JSON change\n",
    "        10, 0, 0,  # position change\n",
    "        0,  # cross-section scaling\n",
    "        0.2, 0, 0,  # orientation change in Euler angles, which is better for a model to learn or a human to understand\n",
    "        2000  # projection scaling (log-scale in neuroglancer)\n",
    "        ]\n",
    "    agent.apply_actions(action_vector, json_state=json_state, euler_angles=True, verbose=False)\n",
    "\n",
    "# If you need to directly change the JSON state, suppose adding a new layer, you can do it with the following function:\n",
    "# agent.chrome_ngl.change_JSON_state(json_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want we can follow an episode recorded by a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  1 Action:  Action Event: Drag\n",
      "Step:  2 Action:  Action Event: Drag\n",
      "Step:  3 Action:  Action Event: Drag\n",
      "Step:  4 Action:  Action Event: Drag\n",
      "Step:  5 Action:  Action Event: Drag\n",
      "Step:  6 Action:  Action Event: Drag\n",
      "Step:  7 Action:  Action Event: Drag\n",
      "Step:  8 Action:  Action Event: Drag\n",
      "Step:  9 Action:  Action Event: Drag\n",
      "Step:  10 Action:  Action Event: Drag\n",
      "Step:  11 Action:  Action Event: Drag\n",
      "Step:  12 Action:  Action Event: Drag\n",
      "Step:  13 Action:  Action Event: Drag\n",
      "Step:  14 Action:  Action Event: Inside render: Single Click: Right Click | Relative position: x=1463, y=205 with keys: None \n",
      "Step:  15 Action:  Action Event: Drag\n",
      "Step:  16 Action:  Action Event: Drag\n",
      "Step:  17 Action:  Action Event: Drag\n",
      "Step:  18 Action:  Action Event: Drag\n",
      "Step:  19 Action:  Action Event: Drag\n",
      "Step:  20 Action:  Action Event: Drag\n",
      "Step:  21 Action:  Action Event: Drag\n",
      "Step:  22 Action:  Action Event: Drag\n",
      "Step:  23 Action:  Action Event: Drag\n",
      "Step:  24 Action:  Action Event: Drag\n",
      "Step:  25 Action:  Action Event: Drag\n",
      "Step:  26 Action:  Action Event: Drag\n",
      "Step:  27 Action:  Action Event: Drag\n",
      "Step:  28 Action:  Action Event: Drag\n",
      "Step:  29 Action:  Action Event: Drag\n",
      "Step:  30 Action:  Action Event: Inside render: Single Click: Right Click | Relative position: x=1614, y=99 with keys: None \n",
      "Step:  31 Action:  Action Event: Drag\n",
      "Step:  32 Action:  Action Event: Drag\n",
      "Step:  33 Action:  Action Event: Drag\n",
      "Step:  34 Action:  Action Event: Drag\n",
      "Step:  35 Action:  Action Event: Drag\n",
      "Step:  36 Action:  Action Event: Inside render: Single Click: Right Click | Relative position: x=1431, y=110 with keys: None \n",
      "Step:  37 Action:  Action Event: Inside render: Single Click: Right Click | Relative position: x=1375, y=142 with keys: None \n",
      "Step:  38 Action:  Action Event: Inside render: Single Click: Right Click | Relative position: x=1428, y=215 with keys: None \n",
      "Step:  39 Action:  Action Event: Inside render: Single Click: Right Click | Relative position: x=1610, y=233 with keys: None \n",
      "Step:  40 Action:  Action Event: Drag\n",
      "Step:  41 Action:  Action Event: Drag\n",
      "Step:  42 Action:  Action Event: Drag\n",
      "Step:  43 Action:  Action Event: Wheel\n",
      "Step:  44 Action:  Action Event: Wheel\n",
      "Step:  45 Action:  Action Event: Wheel\n",
      "Step:  46 Action:  Action Event: Wheel\n",
      "Step:  47 Action:  Action Event: Drag\n",
      "Step:  48 Action:  Action Event: Drag\n",
      "Step:  49 Action:  Action Event: Drag\n",
      "Step:  50 Action:  Action Event: Drag\n",
      "Step:  51 Action:  Action Event: Drag\n",
      "Step:  52 Action:  Action Event: Drag\n",
      "Step:  53 Action:  Action Event: Drag\n",
      "Step:  54 Action:  Action Event: Drag\n",
      "Step:  55 Action:  Action Event: Drag\n",
      "Step:  56 Action:  Action Event: Drag\n",
      "Step:  57 Action:  Action Event: Drag\n",
      "Step:  58 Action:  Action Event: Drag\n",
      "Step:  59 Action:  Action Event: Drag\n",
      "Step:  60 Action:  Action Event: Drag\n",
      "Step:  61 Action:  Action Event: Drag\n",
      "Step:  62 Action:  Action Event: Drag\n",
      "Step:  63 Action:  Action Event: Inside render: Single Click: Right Click | Relative position: x=1259, y=144 with keys: None \n",
      "Step:  64 Action:  Action Event: Drag\n",
      "Step:  65 Action:  Action Event: Drag\n",
      "Step:  66 Action:  Action Event: Drag\n",
      "Step:  67 Action:  Action Event: Drag\n",
      "Step:  68 Action:  Action Event: Drag\n",
      "Step:  69 Action:  Action Event: Drag\n",
      "Step:  70 Action:  Action Event: Drag\n",
      "Step:  71 Action:  Action Event: Drag\n",
      "Step:  72 Action:  Action Event: Drag\n",
      "Step:  73 Action:  Action Event: Drag\n",
      "Step:  74 Action:  Action Event: Drag\n",
      "Step:  75 Action:  Action Event: Drag\n",
      "Step:  76 Action:  Action Event: Drag\n",
      "Step:  77 Action:  Action Event: Drag\n",
      "Step:  78 Action:  Action Event: Drag\n",
      "Step:  79 Action:  Action Event: Drag\n",
      "Step:  80 Action:  Action Event: Drag\n",
      "Step:  81 Action:  Action Event: Drag\n",
      "Step:  82 Action:  Action Event: Drag\n",
      "Step:  83 Action:  Action Event: Drag\n",
      "Step:  84 Action:  Action Event: Inside render: Single Click: Right Click | Relative position: x=1133, y=238 with keys: None \n",
      "Step:  85 Action:  Action Event: Drag\n",
      "Step:  86 Action:  Action Event: Drag\n",
      "Step:  87 Action:  Action Event: Drag\n",
      "Step:  88 Action:  Action Event: Drag\n",
      "Step:  89 Action:  Action Event: Drag\n",
      "Step:  90 Action:  Action Event: Drag\n",
      "Step:  91 Action:  Action Event: Drag\n",
      "Step:  92 Action:  Action Event: Drag\n",
      "Step:  93 Action:  Action Event: Drag\n",
      "Step:  94 Action:  Action Event: Drag\n",
      "Step:  95 Action:  Action Event: Drag\n",
      "Step:  96 Action:  Action Event: Drag\n",
      "Step:  97 Action:  Action Event: Drag\n",
      "Step:  98 Action:  Action Event: Drag\n",
      "Step:  99 Action:  Action Event: Drag\n",
      "Step:  100 Action:  Action Event: Drag\n",
      "Step:  101 Action:  Action Event: Drag\n",
      "Step:  102 Action:  Action Event: Inside render: Single Click: Right Click | Relative position: x=1313, y=431 with keys: None \n",
      "Step:  103 Action:  Action Event: Drag\n",
      "Step:  104 Action:  Action Event: Drag\n",
      "Step:  105 Action:  Action Event: Drag\n",
      "Step:  106 Action:  Action Event: Drag\n",
      "Step:  107 Action:  Action Event: Drag\n",
      "Step:  108 Action:  Action Event: Drag\n",
      "Step:  109 Action:  Action Event: Drag\n",
      "Step:  110 Action:  Action Event: Drag\n",
      "Step:  111 Action:  Action Event: Drag\n",
      "Step:  112 Action:  Action Event: Inside render: Double Click: Left Click | Relative position: x=613, y=458 with keys: None \n",
      "Step:  113 Action:  Action Event: Drag\n",
      "Step:  114 Action:  Action Event: Inside render: Double Click: Left Click | Relative position: x=484, y=555 with keys: None \n",
      "Step:  115 Action:  Action Event: Inside render: Double Click: Left Click | Relative position: x=419, y=574 with keys: None \n"
     ]
    }
   ],
   "source": [
    "episode_path = \"./episodes/raw/1800x900/episode_5.json\"\n",
    "agent.follow_episode(episode_path, sleep_time=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to parse an episode from its recorded sequence of JSON states and actions, we can use the parse_episode function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_path = \"./episodes/raw/1800x900/episode_5.json\"\n",
    "save_path = \"./reparsed_episodes/test/\"\n",
    "parsed_data = agent.parse_episode(episode_path, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create synthetic data by running the following function and adapting the code in the file to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python utils/datagen/collect.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Framework: --WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An environment step is defined as the sequence of actions:\n",
    "# 1. get the state\n",
    "# 2. make decision and apply the action\n",
    "# 3. get the next state\n",
    "# 4. compute the reward\n",
    "# 5. update the policy\n",
    "# -----> \n",
    "#new_pos_state, new_curr_image, new_json_state = agent.prepare_state(image_path=None, euler_angles=False, resize=False, add_mouse=False)\n",
    "#agent.apply_actions(action_vector)\n",
    "# update policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlling Neuroglancer from a cluster with a Hosting machine. --WIP\n",
    "\n",
    "Use proxy/startHost.py to start the hosting machine.\n",
    "\n",
    "Start the cluster script and instead of using the usual functions, you write the action vectors onto a file which is read by the hosting machine. This is experimental and has synchronization issues where the cluster does not wait for the hosting machine to read the action vectors and return the states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
